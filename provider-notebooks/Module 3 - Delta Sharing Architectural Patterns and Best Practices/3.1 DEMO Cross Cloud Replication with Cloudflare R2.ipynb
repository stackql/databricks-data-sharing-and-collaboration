{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 DEMO: Cross Cloud Replication with Cloudflare R2 \\[Provider]\n",
    "\n",
    "## Overview\n",
    "This demo showcases a hybrid data replication pattern using Change Data Feed (CDF) and Cloudflare R2 for cross-cloud/cross-region collaboration. This pattern is particularly useful when:\n",
    "\n",
    "- Recipients need low-latency access to data in their region\n",
    "- Organizations want to minimize egress costs\n",
    "- Data needs to be available even when the provider's workspace is offline\n",
    "- Recipients prefer direct storage access over Delta Sharing protocol\n",
    "\n",
    "**Provider Notebook (This Notebook):** Set up source data with CDF enabled, create an external location using Cloudflare R2, and configure automated replication using MERGE INTO operations based on change data.\n",
    "\n",
    "**Recipient Notebook:** Access the replicated data directly from R2 storage, demonstrate query performance, and compare with traditional Delta Sharing approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "By the end of this demo, you will be able to:\n",
    "1. Enable and use Change Data Feed (CDF) to track table changes\n",
    "2. Configure Cloudflare R2 as an external storage location in Databricks\n",
    "3. Implement incremental replication using CDF and MERGE INTO\n",
    "4. Schedule automated replication jobs for continuous sync\n",
    "5. Monitor replication status and troubleshoot issues\n",
    "6. Understand trade-offs between Delta Sharing and storage-based replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**Scenario:**\n",
    "You are a data provider (\"Global Analytics Corp\") with customer transaction data in AWS US-East. You need to share this data with regional partners in Europe and Asia-Pacific, but they require:\n",
    "- Low-latency access in their regions\n",
    "- Ability to access data even when your Databricks workspace is down\n",
    "- Cost-effective replication without high egress fees\n",
    "\n",
    "**Solution:**\n",
    "Instead of direct Delta Sharing, you'll use Cloudflare R2 (S3-compatible, zero-egress storage) as a replication target. Your source tables use Change Data Feed to track all modifications, and a scheduled job incrementally replicates changes to R2 storage where recipients can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Includes/Demo-Setup-3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Source Table with Change Data Feed\n",
    "\n",
    "Change Data Feed (CDF) tracks all changes (inserts, updates, deletes) to a Delta table. We'll create a transactions table with CDF enabled to capture all modifications for replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create source transactions table with CDF enabled\n",
    "CREATE TABLE IF NOT EXISTS global_analytics.transactions (\n",
    "  transaction_id STRING,\n",
    "  customer_id STRING,\n",
    "  product_id STRING,\n",
    "  amount DECIMAL(10,2),\n",
    "  transaction_date TIMESTAMP,\n",
    "  region STRING,\n",
    "  status STRING\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true'\n",
    ")\n",
    "COMMENT 'Source transactions table with CDF for replication';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Insert initial transaction data\n",
    "INSERT INTO global_analytics.transactions VALUES\n",
    "  ('TXN001', 'CUST001', 'PROD100', 150.00, '2025-01-15 10:30:00', 'US', 'completed'),\n",
    "  ('TXN002', 'CUST002', 'PROD101', 275.50, '2025-01-15 11:15:00', 'EU', 'completed'),\n",
    "  ('TXN003', 'CUST003', 'PROD102', 89.99, '2025-01-15 12:00:00', 'APAC', 'completed'),\n",
    "  ('TXN004', 'CUST004', 'PROD100', 150.00, '2025-01-15 13:45:00', 'US', 'pending'),\n",
    "  ('TXN005', 'CUST005', 'PROD103', 420.00, '2025-01-15 14:20:00', 'EU', 'completed');\n",
    "\n",
    "SELECT * FROM global_analytics.transactions ORDER BY transaction_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify CDF is enabled\n",
    "DESCRIBE DETAIL global_analytics.transactions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Cloudflare R2 External Location\n",
    "\n",
    "Cloudflare R2 provides S3-compatible object storage with zero egress fees. We'll configure it as an external location in Unity Catalog. You need:\n",
    "- R2 bucket name\n",
    "- R2 access key ID\n",
    "- R2 secret access key\n",
    "- R2 endpoint URL (account-specific)\n",
    "\n",
    "**Note:** In production, credentials should be stored in Databricks secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create storage credential for R2 (Metastore admin required)\n",
    "-- Replace with your actual R2 credentials\n",
    "CREATE STORAGE CREDENTIAL IF NOT EXISTS r2_storage_credential\n",
    "WITH (AWS_IAM_ROLE=<your-r2-credentials>)\n",
    "COMMENT 'Cloudflare R2 storage credentials for cross-cloud replication';\n",
    "\n",
    "-- Create external location pointing to R2\n",
    "-- Replace <account-id> with your Cloudflare account ID\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS r2_replication_location\n",
    "URL 's3://global-analytics-replication/<account-id>.r2.cloudflarestorage.com/'\n",
    "WITH (STORAGE CREDENTIAL r2_storage_credential)\n",
    "COMMENT 'Cloudflare R2 location for replicated transaction data';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative Setup Using Python (with Secrets):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure R2 credentials from secrets\n",
    "r2_access_key = dbutils.secrets.get(scope=\"r2-secrets\", key=\"access-key-id\")\n",
    "r2_secret_key = dbutils.secrets.get(scope=\"r2-secrets\", key=\"secret-access-key\")\n",
    "r2_endpoint = dbutils.secrets.get(scope=\"r2-secrets\", key=\"endpoint-url\")\n",
    "\n",
    "# Configure Spark to use R2\n",
    "spark.conf.set(\"fs.s3a.access.key\", r2_access_key)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", r2_secret_key)\n",
    "spark.conf.set(\"fs.s3a.endpoint\", r2_endpoint)\n",
    "spark.conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "# Test connection\n",
    "r2_path = \"s3a://global-analytics-replication/transactions/\"\n",
    "print(f\"R2 storage configured at: {r2_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Target External Table on R2\n",
    "\n",
    "The target table will be an external Delta table stored in Cloudflare R2. This allows recipients to access it directly from their cloud/region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create external table in R2 using external location\n",
    "CREATE TABLE IF NOT EXISTS global_analytics.transactions_r2_replica\n",
    "(\n",
    "  transaction_id STRING,\n",
    "  customer_id STRING,\n",
    "  product_id STRING,\n",
    "  amount DECIMAL(10,2),\n",
    "  transaction_date TIMESTAMP,\n",
    "  region STRING,\n",
    "  status STRING,\n",
    "  _replicated_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION 's3://global-analytics-replication/transactions/'\n",
    "COMMENT 'Replicated transactions in Cloudflare R2 for cross-cloud access';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initial Data Load to R2\n",
    "\n",
    "Perform the initial full copy of data to R2 storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Initial data load with replication timestamp\n",
    "INSERT INTO global_analytics.transactions_r2_replica\n",
    "SELECT \n",
    "  transaction_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  amount,\n",
    "  transaction_date,\n",
    "  region,\n",
    "  status,\n",
    "  current_timestamp() as _replicated_at\n",
    "FROM global_analytics.transactions;\n",
    "\n",
    "-- Verify initial load\n",
    "SELECT COUNT(*) as replica_count FROM global_analytics.transactions_r2_replica;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement Incremental Replication Using CDF\n",
    "\n",
    "Now we'll create a replication process that:\n",
    "1. Reads changes from the source table using CDF\n",
    "2. Applies those changes to the R2 replica using MERGE INTO\n",
    "3. Tracks the last replicated version for incremental processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metadata table to track replication progress\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS global_analytics.replication_metadata (\n",
    "  source_table STRING,\n",
    "  target_table STRING,\n",
    "  last_replicated_version LONG,\n",
    "  last_replicated_timestamp TIMESTAMP,\n",
    "  replication_status STRING\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Initialize metadata\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO global_analytics.replication_metadata AS target\n",
    "USING (\n",
    "  SELECT \n",
    "    'global_analytics.transactions' as source_table,\n",
    "    'global_analytics.transactions_r2_replica' as target_table,\n",
    "    0 as last_replicated_version,\n",
    "    current_timestamp() as last_replicated_timestamp,\n",
    "    'initialized' as replication_status\n",
    ") AS source\n",
    "ON target.source_table = source.source_table\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_changes():\n",
    "    \"\"\"\n",
    "    Incremental replication function using Change Data Feed.\n",
    "    Reads changes since last replication and applies to R2 replica.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import col, current_timestamp, lit\n",
    "    \n",
    "    # Get last replicated version\n",
    "    last_version_df = spark.sql(\"\"\"\n",
    "        SELECT last_replicated_version \n",
    "        FROM global_analytics.replication_metadata\n",
    "        WHERE source_table = 'global_analytics.transactions'\n",
    "    \"\"\")\n",
    "    \n",
    "    last_version = last_version_df.collect()[0][0]\n",
    "    \n",
    "    # Get current version of source table\n",
    "    current_version = spark.sql(\n",
    "        \"DESCRIBE HISTORY global_analytics.transactions LIMIT 1\"\n",
    "    ).select(\"version\").collect()[0][0]\n",
    "    \n",
    "    print(f\"Replicating changes from version {last_version + 1} to {current_version}\")\n",
    "    \n",
    "    if current_version <= last_version:\n",
    "        print(\"No new changes to replicate\")\n",
    "        return\n",
    "    \n",
    "    # Read changes using CDF\n",
    "    changes_df = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"readChangeFeed\", \"true\") \\\n",
    "        .option(\"startingVersion\", last_version + 1) \\\n",
    "        .option(\"endingVersion\", current_version) \\\n",
    "        .table(\"global_analytics.transactions\")\n",
    "    \n",
    "    changes_count = changes_df.count()\n",
    "    print(f\"Found {changes_count} change records to process\")\n",
    "    \n",
    "    if changes_count == 0:\n",
    "        return\n",
    "    \n",
    "    # Create temp view for merge\n",
    "    changes_df.createOrReplaceTempView(\"changes_temp\")\n",
    "    \n",
    "    # Apply changes to R2 replica using MERGE\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO global_analytics.transactions_r2_replica AS target\n",
    "        USING (\n",
    "            SELECT \n",
    "                transaction_id,\n",
    "                customer_id,\n",
    "                product_id,\n",
    "                amount,\n",
    "                transaction_date,\n",
    "                region,\n",
    "                status,\n",
    "                _change_type,\n",
    "                _commit_version\n",
    "            FROM (\n",
    "                SELECT *,\n",
    "                    ROW_NUMBER() OVER (\n",
    "                        PARTITION BY transaction_id \n",
    "                        ORDER BY _commit_version DESC, _commit_timestamp DESC\n",
    "                    ) as rn\n",
    "                FROM changes_temp\n",
    "            )\n",
    "            WHERE rn = 1\n",
    "        ) AS source\n",
    "        ON target.transaction_id = source.transaction_id\n",
    "        WHEN MATCHED AND source._change_type = 'update_postimage' THEN\n",
    "            UPDATE SET\n",
    "                customer_id = source.customer_id,\n",
    "                product_id = source.product_id,\n",
    "                amount = source.amount,\n",
    "                transaction_date = source.transaction_date,\n",
    "                region = source.region,\n",
    "                status = source.status,\n",
    "                _replicated_at = current_timestamp()\n",
    "        WHEN MATCHED AND source._change_type = 'delete' THEN DELETE\n",
    "        WHEN NOT MATCHED AND source._change_type IN ('insert', 'update_postimage') THEN\n",
    "            INSERT (\n",
    "                transaction_id, customer_id, product_id, amount,\n",
    "                transaction_date, region, status, _replicated_at\n",
    "            )\n",
    "            VALUES (\n",
    "                source.transaction_id, source.customer_id, source.product_id,\n",
    "                source.amount, source.transaction_date, source.region,\n",
    "                source.status, current_timestamp()\n",
    "            )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Update metadata\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE global_analytics.replication_metadata\n",
    "        SET \n",
    "            last_replicated_version = {current_version},\n",
    "            last_replicated_timestamp = current_timestamp(),\n",
    "            replication_status = 'success'\n",
    "        WHERE source_table = 'global_analytics.transactions'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"Successfully replicated {changes_count} changes to R2\")\n",
    "\n",
    "# Test the replication function\n",
    "replicate_changes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Replication with Data Changes\n",
    "\n",
    "Let's make some changes to the source table and verify they replicate correctly to R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Insert new transactions\n",
    "INSERT INTO global_analytics.transactions VALUES\n",
    "  ('TXN006', 'CUST006', 'PROD104', 325.00, '2025-01-16 09:00:00', 'APAC', 'completed'),\n",
    "  ('TXN007', 'CUST007', 'PROD100', 150.00, '2025-01-16 10:30:00', 'US', 'completed');\n",
    "\n",
    "-- Update existing transaction\n",
    "UPDATE global_analytics.transactions\n",
    "SET status = 'completed'\n",
    "WHERE transaction_id = 'TXN004';\n",
    "\n",
    "-- Delete a transaction\n",
    "DELETE FROM global_analytics.transactions\n",
    "WHERE transaction_id = 'TXN001';\n",
    "\n",
    "SELECT * FROM global_analytics.transactions ORDER BY transaction_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run replication to apply changes\n",
    "replicate_changes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Verify changes in R2 replica\n",
    "SELECT 'Source' as table_name, COUNT(*) as record_count \n",
    "FROM global_analytics.transactions\n",
    "UNION ALL\n",
    "SELECT 'R2 Replica' as table_name, COUNT(*) as record_count \n",
    "FROM global_analytics.transactions_r2_replica;\n",
    "\n",
    "-- Check specific records\n",
    "SELECT * FROM global_analytics.transactions_r2_replica \n",
    "ORDER BY transaction_date;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Schedule Automated Replication Job\n",
    "\n",
    "In production, you'll want to schedule the replication to run automatically. Here's how to create a Databricks job for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a job definition that can be deployed\n",
    "# In practice, you would use Databricks CLI or REST API to create the job\n",
    "\n",
    "job_config = {\n",
    "    \"name\": \"R2 Replication - Transactions\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"replicate_transactions\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/path/to/this/notebook\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"job_cluster_key\": \"replication_cluster\",\n",
    "            \"timeout_seconds\": 3600,\n",
    "            \"max_retries\": 2\n",
    "        }\n",
    "    ],\n",
    "    \"job_clusters\": [\n",
    "        {\n",
    "            \"job_cluster_key\": \"replication_cluster\",\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"i3.xlarge\",\n",
    "                \"num_workers\": 2\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 */15 * * * ?\",  # Every 15 minutes\n",
    "        \"timezone_id\": \"UTC\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Job configuration for R2 replication:\")\n",
    "print(job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Monitor Replication Status\n",
    "\n",
    "Create monitoring queries to track replication health and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Check replication metadata\n",
    "SELECT \n",
    "    source_table,\n",
    "    target_table,\n",
    "    last_replicated_version,\n",
    "    last_replicated_timestamp,\n",
    "    replication_status,\n",
    "    TIMESTAMPDIFF(MINUTE, last_replicated_timestamp, current_timestamp()) as minutes_since_last_replication\n",
    "FROM global_analytics.replication_metadata;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Compare record counts between source and replica\n",
    "WITH source_stats AS (\n",
    "    SELECT \n",
    "        COUNT(*) as record_count,\n",
    "        MAX(transaction_date) as latest_transaction,\n",
    "        SUM(amount) as total_amount\n",
    "    FROM global_analytics.transactions\n",
    "),\n",
    "replica_stats AS (\n",
    "    SELECT \n",
    "        COUNT(*) as record_count,\n",
    "        MAX(transaction_date) as latest_transaction,\n",
    "        SUM(amount) as total_amount,\n",
    "        MAX(_replicated_at) as last_replication\n",
    "    FROM global_analytics.transactions_r2_replica\n",
    ")\n",
    "SELECT \n",
    "    'Source' as table_type,\n",
    "    source_stats.*,\n",
    "    NULL as last_replication\n",
    "FROM source_stats\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'R2 Replica' as table_type,\n",
    "    replica_stats.*\n",
    "FROM replica_stats;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Check for replication lag\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.transaction_date as source_date,\n",
    "    r._replicated_at as replica_date,\n",
    "    CASE \n",
    "        WHEN r.transaction_id IS NULL THEN 'Missing in replica'\n",
    "        ELSE 'Replicated'\n",
    "    END as replication_status\n",
    "FROM global_analytics.transactions t\n",
    "LEFT JOIN global_analytics.transactions_r2_replica r\n",
    "    ON t.transaction_id = r.transaction_id\n",
    "ORDER BY t.transaction_date DESC\n",
    "LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Performance Optimization Tips\n",
    "\n",
    "Here are some optimizations for production deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Optimize R2 replica table for query performance\n",
    "OPTIMIZE global_analytics.transactions_r2_replica\n",
    "ZORDER BY (region, transaction_date);\n",
    "\n",
    "-- Vacuum old versions (keep 7 days for time travel)\n",
    "VACUUM global_analytics.transactions_r2_replica RETAIN 168 HOURS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure partition strategy for large tables\n",
    "# This example partitions by date for efficient time-based queries\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS global_analytics.transactions_r2_replica_partitioned\n",
    "(\n",
    "  transaction_id STRING,\n",
    "  customer_id STRING,\n",
    "  product_id STRING,\n",
    "  amount DECIMAL(10,2),\n",
    "  transaction_date TIMESTAMP,\n",
    "  region STRING,\n",
    "  status STRING,\n",
    "  _replicated_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (DATE(transaction_date))\n",
    "LOCATION 's3://global-analytics-replication/transactions_partitioned/'\n",
    "COMMENT 'Partitioned replica for better query performance'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Sharing R2 Location with Recipients\n",
    "\n",
    "To enable recipients to access the R2 data, share the following information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recipient access instructions\n",
    "recipient_instructions = f\"\"\"\n",
    "=== R2 Data Access Information ===\n",
    "\n",
    "Storage Location: s3://global-analytics-replication/transactions/\n",
    "Endpoint: <your-account-id>.r2.cloudflarestorage.com\n",
    "Region: auto (Cloudflare R2 automatically routes to nearest location)\n",
    "\n",
    "Required Credentials:\n",
    "- Access Key ID: [Provide via secure channel]\n",
    "- Secret Access Key: [Provide via secure channel]\n",
    "\n",
    "Data Format: Delta Lake\n",
    "Update Frequency: Every 15 minutes\n",
    "Replication Lag: Typically < 5 minutes\n",
    "\n",
    "Table Schema:\n",
    "  - transaction_id: STRING (Primary Key)\n",
    "  - customer_id: STRING\n",
    "  - product_id: STRING\n",
    "  - amount: DECIMAL(10,2)\n",
    "  - transaction_date: TIMESTAMP\n",
    "  - region: STRING\n",
    "  - status: STRING\n",
    "  - _replicated_at: TIMESTAMP (Replication metadata)\n",
    "\n",
    "Usage Notes:\n",
    "- Data is read-only for recipients\n",
    "- Changes are replicated incrementally\n",
    "- Time travel available for 7 days\n",
    "- No egress charges from Cloudflare R2\n",
    "\"\"\"\n",
    "\n",
    "print(recipient_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **What we accomplished:**\n",
    "\n",
    "1. Created source table with Change Data Feed enabled\n",
    "2. Configured Cloudflare R2 as external storage location\n",
    "3. Implemented incremental replication using CDF and MERGE INTO\n",
    "4. Built monitoring and metadata tracking\n",
    "5. Optimized replica for query performance\n",
    "6. Prepared recipient access instructions\n",
    "\n",
    "**Key Benefits of This Pattern:**\n",
    "- 🌍 **Global Access**: Recipients in any region get low-latency access\n",
    "- 💰 **Cost Effective**: Zero egress fees with Cloudflare R2\n",
    "- ⚡ **Performance**: Direct storage access without protocol overhead\n",
    "- 🔄 **Incremental**: CDF enables efficient change tracking\n",
    "- 🛡️ **Resilient**: Recipients can access data even if provider workspace is offline\n",
    "- 📊 **Flexible**: Recipients can use any Delta Lake compatible tool\n",
    "\n",
    "**Trade-offs vs. Direct Delta Sharing:**\n",
    "- ➕ Lower latency for recipients\n",
    "- ➕ Lower costs (no egress fees)\n",
    "- ➕ Works across any cloud/region\n",
    "- ➖ Replication lag (not real-time)\n",
    "- ➖ Additional storage costs\n",
    "- ➖ More complex setup and maintenance\n",
    "\n",
    "**Next Steps:**\n",
    "Continue to the recipient notebook to see how partners access and query this replicated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
