{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img\n",
        "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
        "    alt=\"Databricks Learning\"\n",
        "  >\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.1 DEMO: Cross Cloud Replication with Cloudflare R2 [Provider]\n",
        "\n",
        "## Overview\n",
        "This demo showcases how to set up cross-cloud replication using Cloudflare R2 as an intermediary storage layer. The provider creates a managed table with Change Data Feed (CDF) enabled, then replicates changes to an external table on R2 for global, cost-effective data sharing.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this demo, you will understand:\n",
        "1. How to enable Change Data Feed on managed tables\n",
        "2. How to configure Cloudflare R2 external storage\n",
        "3. How to replicate table changes to R2 using CDF\n",
        "4. How to automate replication using Databricks Jobs\n",
        "\n",
        "## Architecture\n",
        "```\n",
        "Managed Table (with CDF) ‚Üí CDF Stream ‚Üí External Table (R2) ‚Üí Recipients\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Zero egress costs with Cloudflare R2\n",
        "- Global data distribution without provider dependencies\n",
        "- Automated change propagation using CDF\n",
        "- Cost-effective sharing with unlimited recipients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run the common setup and demo configuration scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./Demo-Setup-3_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configure Cloudflare R2 Storage Credentials\n",
        "\n",
        "First, we need to configure the storage credentials for Cloudflare R2. In production, you would store these as secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# R2 Configuration - Replace with your actual values\n",
        "# In production, use Databricks Secrets for these values\n",
        "r2_endpoint = \"https://4132d7d5587ee99b9d482ecfc2c1853c.r2.cloudflarestorage.com\"\n",
        "r2_bucket = \"databricks-demo\"\n",
        "r2_access_key = \"<your-r2-access-key>\"  # Replace with actual key\n",
        "r2_secret_key = \"<your-r2-secret-key>\"  # Replace with actual key\n",
        "\n",
        "# Construct the full R2 path for our replica table\n",
        "r2_table_path = f\"s3a://{r2_bucket}/{DA.r2_path}\"\n",
        "\n",
        "print(f\"R2 Endpoint: {r2_endpoint}\")\n",
        "print(f\"R2 Bucket: {r2_bucket}\")\n",
        "print(f\"R2 Table Path: {r2_table_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Spark for R2 Access\n",
        "\n",
        "Configure Spark to use the R2 credentials and endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Spark for R2 access\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.endpoint\", r2_endpoint)\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.access.key\", r2_access_key)\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.secret.key\", r2_secret_key)\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.path.style.access\", \"true\")\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.connection.ssl.enabled\", \"true\")\n",
        "\n",
        "print(\"‚úÖ Spark configured for R2 access\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Source Managed Table with Sample Data\n",
        "\n",
        "Create a managed table that will serve as our primary data source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the source transactions table with initial data\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {DA.catalog}.{DA.schema}.{DA.source_table} (\n",
        "  transaction_id STRING,\n",
        "  customer_id STRING,\n",
        "  product_category STRING,\n",
        "  amount DECIMAL(10,2),\n",
        "  transaction_date DATE,\n",
        "  region STRING,\n",
        "  created_at TIMESTAMP\n",
        ")\n",
        "USING DELTA\n",
        "PARTITIONED BY (transaction_date)\n",
        "TBLPROPERTIES (\n",
        "  'delta.enableChangeDataFeed' = 'true',\n",
        "  'delta.autoOptimize.optimizeWrite' = 'true',\n",
        "  'delta.autoOptimize.autoCompact' = 'true'\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "print(f\"‚úÖ Created managed table: {DA.catalog}.{DA.schema}.{DA.source_table}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert initial sample data\n",
        "from datetime import datetime, date\n",
        "import uuid\n",
        "\n",
        "# Generate sample transactions\n",
        "sample_data = []\n",
        "regions = ['North America', 'Europe', 'Asia Pacific', 'Latin America']\n",
        "categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports']\n",
        "\n",
        "for i in range(100):\n",
        "    sample_data.append((\n",
        "        str(uuid.uuid4()),\n",
        "        f\"customer_{i % 50}\",\n",
        "        categories[i % len(categories)],\n",
        "        round(50 + (i * 3.7) % 500, 2),\n",
        "        date(2024, 10, 20 + (i % 7)),\n",
        "        regions[i % len(regions)],\n",
        "        datetime.now()\n",
        "    ))\n",
        "\n",
        "# Create DataFrame and insert\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, DateType, TimestampType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"product_category\", StringType(), True),\n",
        "    StructField(\"amount\", DecimalType(10,2), True),\n",
        "    StructField(\"transaction_date\", DateType(), True),\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"created_at\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(sample_data, schema)\n",
        "df.write.mode(\"append\").saveAsTable(f\"{DA.catalog}.{DA.schema}.{DA.source_table}\")\n",
        "\n",
        "print(f\"‚úÖ Inserted {df.count()} sample transactions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify Change Data Feed is Enabled\n",
        "\n",
        "Check that CDF is properly enabled on our source table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check table properties to confirm CDF is enabled\n",
        "table_details = spark.sql(f\"DESCRIBE DETAIL {DA.catalog}.{DA.schema}.{DA.source_table}\")\n",
        "display(table_details.select(\"name\", \"location\", \"properties\"))\n",
        "\n",
        "# Get current version\n",
        "current_version = table_details.select(\"version\").collect()[0][0]\n",
        "print(f\"\\n‚úÖ Current table version: {current_version}\")\n",
        "print(f\"‚úÖ CDF enabled: Look for 'delta.enableChangeDataFeed' = 'true' in properties\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create External Table on Cloudflare R2\n",
        "\n",
        "Create an external table that will store our replicated data on R2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create external table on R2 with same schema\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {DA.catalog}.{DA.schema}.{DA.replica_table} (\n",
        "  transaction_id STRING,\n",
        "  customer_id STRING,\n",
        "  product_category STRING,\n",
        "  amount DECIMAL(10,2),\n",
        "  transaction_date DATE,\n",
        "  region STRING,\n",
        "  created_at TIMESTAMP\n",
        ")\n",
        "USING DELTA\n",
        "LOCATION '{r2_table_path}'\n",
        "PARTITIONED BY (transaction_date)\n",
        "TBLPROPERTIES (\n",
        "  'delta.autoOptimize.optimizeWrite' = 'true',\n",
        "  'delta.autoOptimize.autoCompact' = 'true'\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "print(f\"‚úÖ Created external table on R2: {DA.catalog}.{DA.schema}.{DA.replica_table}\")\n",
        "print(f\"‚úÖ Location: {r2_table_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initial Data Replication\n",
        "\n",
        "Perform the initial full copy of data to the R2 external table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform initial full replication\n",
        "source_df = spark.table(f\"{DA.catalog}.{DA.schema}.{DA.source_table}\")\n",
        "\n",
        "# Write to external R2 table\n",
        "source_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(f\"{DA.catalog}.{DA.schema}.{DA.replica_table}\")\n",
        "\n",
        "print(f\"‚úÖ Initial replication completed\")\n",
        "print(f\"   Source records: {source_df.count()}\")\n",
        "\n",
        "# Verify replication\n",
        "replica_count = spark.table(f\"{DA.catalog}.{DA.schema}.{DA.replica_table}\").count()\n",
        "print(f\"   Replica records: {replica_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Add More Data to Source Table\n",
        "\n",
        "Simulate ongoing business operations by adding more data to the source table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add new transactions to simulate ongoing operations\n",
        "new_data = []\n",
        "for i in range(100, 150):\n",
        "    new_data.append((\n",
        "        str(uuid.uuid4()),\n",
        "        f\"customer_{i % 60}\",\n",
        "        categories[i % len(categories)],\n",
        "        round(75 + (i * 4.2) % 400, 2),\n",
        "        date(2024, 10, 27),  # Today's date\n",
        "        regions[i % len(regions)],\n",
        "        datetime.now()\n",
        "    ))\n",
        "\n",
        "new_df = spark.createDataFrame(new_data, schema)\n",
        "new_df.write.mode(\"append\").saveAsTable(f\"{DA.catalog}.{DA.schema}.{DA.source_table}\")\n",
        "\n",
        "print(f\"‚úÖ Added {new_df.count()} new transactions\")\n",
        "\n",
        "# Check new version\n",
        "new_version = spark.sql(f\"DESCRIBE DETAIL {DA.catalog}.{DA.schema}.{DA.source_table}\").select(\"version\").collect()[0][0]\n",
        "print(f\"‚úÖ Table version updated to: {new_version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Read Change Data Feed\n",
        "\n",
        "Read the changes from the source table using Change Data Feed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read changes from the last version\n",
        "changes_df = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"readChangeDataFeed\", \"true\") \\\n",
        "    .option(\"startingVersion\", current_version + 1) \\\n",
        "    .table(f\"{DA.catalog}.{DA.schema}.{DA.source_table}\")\n",
        "\n",
        "print(f\"‚úÖ Changes detected: {changes_df.count()} records\")\n",
        "\n",
        "# Show the structure of CDF data\n",
        "print(\"\\nüìä Change Data Feed Structure:\")\n",
        "changes_df.printSchema()\n",
        "\n",
        "# Display sample changes\n",
        "print(\"\\nüìÑ Sample Changes:\")\n",
        "display(changes_df.select(\"transaction_id\", \"amount\", \"region\", \"_change_type\", \"_commit_version\").limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Apply Changes to R2 Replica\n",
        "\n",
        "Apply the detected changes to our R2 external table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter only the data changes (ignore metadata)\n",
        "data_changes = changes_df.filter(\"_change_type = 'insert'\")\n",
        "\n",
        "if data_changes.count() > 0:\n",
        "    # Select only the business columns (exclude CDF metadata)\n",
        "    business_data = data_changes.select(\n",
        "        \"transaction_id\", \"customer_id\", \"product_category\", \n",
        "        \"amount\", \"transaction_date\", \"region\", \"created_at\"\n",
        "    )\n",
        "    \n",
        "    # Append changes to R2 table\n",
        "    business_data.write \\\n",
        "        .mode(\"append\") \\\n",
        "        .saveAsTable(f\"{DA.catalog}.{DA.schema}.{DA.replica_table}\")\n",
        "    \n",
        "    print(f\"‚úÖ Applied {business_data.count()} changes to R2 replica\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  No data changes to apply\")\n",
        "\n",
        "# Verify replication is in sync\n",
        "source_count = spark.table(f\"{DA.catalog}.{DA.schema}.{DA.source_table}\").count()\n",
        "replica_count = spark.table(f\"{DA.catalog}.{DA.schema}.{DA.replica_table}\").count()\n",
        "\n",
        "print(f\"\\nüìä Replication Status:\")\n",
        "print(f\"   Source table: {source_count} records\")\n",
        "print(f\"   R2 replica:   {replica_count} records\")\n",
        "print(f\"   In sync: {'‚úÖ Yes' if source_count == replica_count else '‚ùå No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Create Replication Function\n",
        "\n",
        "Create a reusable function for ongoing replication that can be scheduled as a job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replicate_changes_to_r2(source_table, replica_table, last_processed_version=None):\n",
        "    \"\"\"\n",
        "    Replicate changes from source table to R2 replica using Change Data Feed.\n",
        "    \n",
        "    Args:\n",
        "        source_table: Fully qualified source table name\n",
        "        replica_table: Fully qualified replica table name\n",
        "        last_processed_version: Last version that was processed (optional)\n",
        "    \n",
        "    Returns:\n",
        "        dict: Statistics about the replication process\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get current version of source table\n",
        "        current_version = spark.sql(f\"DESCRIBE DETAIL {source_table}\").select(\"version\").collect()[0][0]\n",
        "        \n",
        "        # If no last_processed_version provided, get it from replica table properties\n",
        "        if last_processed_version is None:\n",
        "            try:\n",
        "                replica_props = spark.sql(f\"DESCRIBE DETAIL {replica_table}\").select(\"properties\").collect()[0][0]\n",
        "                last_processed_version = int(replica_props.get('last_processed_version', '0'))\n",
        "            except:\n",
        "                last_processed_version = 0\n",
        "        \n",
        "        starting_version = last_processed_version + 1\n",
        "        \n",
        "        print(f\"üîÑ Checking for changes from version {starting_version} to {current_version}\")\n",
        "        \n",
        "        if starting_version > current_version:\n",
        "            return {\n",
        "                'status': 'up_to_date',\n",
        "                'records_processed': 0,\n",
        "                'current_version': current_version,\n",
        "                'message': 'Replica is up to date'\n",
        "            }\n",
        "        \n",
        "        # Read changes using CDF\n",
        "        changes_df = spark.read \\\n",
        "            .format(\"delta\") \\\n",
        "            .option(\"readChangeDataFeed\", \"true\") \\\n",
        "            .option(\"startingVersion\", starting_version) \\\n",
        "            .table(source_table)\n",
        "        \n",
        "        # Process different change types\n",
        "        inserts = changes_df.filter(\"_change_type = 'insert'\")\n",
        "        updates_pre = changes_df.filter(\"_change_type = 'update_preimage'\")\n",
        "        updates_post = changes_df.filter(\"_change_type = 'update_postimage'\")\n",
        "        deletes = changes_df.filter(\"_change_type = 'delete'\")\n",
        "        \n",
        "        total_changes = 0\n",
        "        \n",
        "        # Apply inserts\n",
        "        if inserts.count() > 0:\n",
        "            business_data = inserts.select(\n",
        "                \"transaction_id\", \"customer_id\", \"product_category\",\n",
        "                \"amount\", \"transaction_date\", \"region\", \"created_at\"\n",
        "            )\n",
        "            business_data.write.mode(\"append\").saveAsTable(replica_table)\n",
        "            insert_count = business_data.count()\n",
        "            total_changes += insert_count\n",
        "            print(f\"   ‚úÖ Applied {insert_count} inserts\")\n",
        "        \n",
        "        # For this demo, we'll focus on inserts. In production, you'd handle\n",
        "        # updates and deletes using MERGE statements\n",
        "        \n",
        "        # Update replica table properties with last processed version\n",
        "        spark.sql(f\"\"\"\n",
        "            ALTER TABLE {replica_table} \n",
        "            SET TBLPROPERTIES ('last_processed_version' = '{current_version}')\n",
        "        \"\"\")\n",
        "        \n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'records_processed': total_changes,\n",
        "            'current_version': current_version,\n",
        "            'starting_version': starting_version,\n",
        "            'message': f'Successfully replicated {total_changes} changes'\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'records_processed': 0,\n",
        "            'error': str(e),\n",
        "            'message': f'Replication failed: {str(e)}'\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Replication function created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Test the Replication Function\n",
        "\n",
        "Test our replication function with additional data changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add more test data\n",
        "test_data = []\n",
        "for i in range(200, 220):\n",
        "    test_data.append((\n",
        "        str(uuid.uuid4()),\n",
        "        f\"customer_{i % 70}\",\n",
        "        categories[i % len(categories)],\n",
        "        round(100 + (i * 2.5) % 300, 2),\n",
        "        date(2024, 10, 27),\n",
        "        regions[i % len(regions)],\n",
        "        datetime.now()\n",
        "    ))\n",
        "\n",
        "test_df = spark.createDataFrame(test_data, schema)\n",
        "test_df.write.mode(\"append\").saveAsTable(f\"{DA.catalog}.{DA.schema}.{DA.source_table}\")\n",
        "\n",
        "print(f\"‚úÖ Added {test_df.count()} test transactions\")\n",
        "\n",
        "# Test the replication function\n",
        "result = replicate_changes_to_r2(\n",
        "    f\"{DA.catalog}.{DA.schema}.{DA.source_table}\",\n",
        "    f\"{DA.catalog}.{DA.schema}.{DA.replica_table}\"\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Replication Result:\")\n",
        "print(f\"   Status: {result['status']}\")\n",
        "print(f\"   Records processed: {result['records_processed']}\")\n",
        "print(f\"   Message: {result['message']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Verify Replication and Data Quality\n",
        "\n",
        "Perform final verification that our replication is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare source and replica data\n",
        "source_stats = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(DISTINCT transaction_id) as unique_transactions,\n",
        "        ROUND(SUM(amount), 2) as total_amount,\n",
        "        MAX(transaction_date) as latest_date\n",
        "    FROM {DA.catalog}.{DA.schema}.{DA.source_table}\n",
        "\"\"\").collect()[0]\n",
        "\n",
        "replica_stats = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(DISTINCT transaction_id) as unique_transactions,\n",
        "        ROUND(SUM(amount), 2) as total_amount,\n",
        "        MAX(transaction_date) as latest_date\n",
        "    FROM {DA.catalog}.{DA.schema}.{DA.replica_table}\n",
        "\"\"\").collect()[0]\n",
        "\n",
        "print(\"üìä Data Quality Verification:\")\n",
        "print(f\"\\n   Source Table ({DA.source_table}):\")\n",
        "print(f\"     Records: {source_stats[0]}\")\n",
        "print(f\"     Unique transactions: {source_stats[1]}\")\n",
        "print(f\"     Total amount: ${source_stats[2]}\")\n",
        "print(f\"     Latest date: {source_stats[3]}\")\n",
        "\n",
        "print(f\"\\n   R2 Replica ({DA.replica_table}):\")\n",
        "print(f\"     Records: {replica_stats[0]}\")\n",
        "print(f\"     Unique transactions: {replica_stats[1]}\")\n",
        "print(f\"     Total amount: ${replica_stats[2]}\")\n",
        "print(f\"     Latest date: {replica_stats[3]}\")\n",
        "\n",
        "# Verify data integrity\n",
        "data_match = (\n",
        "    source_stats[0] == replica_stats[0] and\n",
        "    source_stats[1] == replica_stats[1] and\n",
        "    source_stats[2] == replica_stats[2]\n",
        ")\n",
        "\n",
        "print(f\"\\n   Data Integrity: {'‚úÖ PASSED' if data_match else '‚ùå FAILED'}\")\n",
        "\n",
        "if data_match:\n",
        "    print(\"   üéâ Replication is working perfectly!\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Data mismatch detected - check replication logic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Create Job-Ready Replication Script\n",
        "\n",
        "Create a production-ready script that can be scheduled as a Databricks Job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Production replication script\n",
        "def production_replication_job():\n",
        "    \"\"\"\n",
        "    Production-ready replication job that can be scheduled.\n",
        "    This function includes error handling, logging, and monitoring.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    \n",
        "    job_start = datetime.now()\n",
        "    \n",
        "    try:\n",
        "        print(f\"üîÑ Starting replication job at {job_start}\")\n",
        "        \n",
        "        # Define table names (in production, these could be parameters)\n",
        "        source_table = f\"{DA.catalog}.{DA.schema}.{DA.source_table}\"\n",
        "        replica_table = f\"{DA.catalog}.{DA.schema}.{DA.replica_table}\"\n",
        "        \n",
        "        # Run replication\n",
        "        result = replicate_changes_to_r2(source_table, replica_table)\n",
        "        \n",
        "        job_end = datetime.now()\n",
        "        duration = (job_end - job_start).total_seconds()\n",
        "        \n",
        "        # Log results\n",
        "        log_entry = {\n",
        "            'timestamp': job_start.isoformat(),\n",
        "            'duration_seconds': duration,\n",
        "            'status': result['status'],\n",
        "            'records_processed': result['records_processed'],\n",
        "            'source_table': source_table,\n",
        "            'replica_table': replica_table\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n‚úÖ Job completed successfully\")\n",
        "        print(f\"   Duration: {duration:.2f} seconds\")\n",
        "        print(f\"   Records processed: {result['records_processed']}\")\n",
        "        print(f\"   Status: {result['status']}\")\n",
        "        \n",
        "        # In production, you might want to:\n",
        "        # 1. Send metrics to monitoring system\n",
        "        # 2. Log to centralized logging system\n",
        "        # 3. Send alerts on failure\n",
        "        # 4. Update job status in metadata table\n",
        "        \n",
        "        return log_entry\n",
        "        \n",
        "    except Exception as e:\n",
        "        job_end = datetime.now()\n",
        "        duration = (job_end - job_start).total_seconds()\n",
        "        \n",
        "        error_log = {\n",
        "            'timestamp': job_start.isoformat(),\n",
        "            'duration_seconds': duration,\n",
        "            'status': 'error',\n",
        "            'error_message': str(e),\n",
        "            'source_table': source_table,\n",
        "            'replica_table': replica_table\n",
        "        }\n",
        "        \n",
        "        print(f\"‚ùå Job failed after {duration:.2f} seconds\")\n",
        "        print(f\"   Error: {str(e)}\")\n",
        "        \n",
        "        # In production, send alert to operations team\n",
        "        raise\n",
        "\n",
        "# Test the production job\n",
        "job_result = production_replication_job()\n",
        "print(f\"\\nüìã Job Result: {job_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Monitoring and Observability\n",
        "\n",
        "Set up monitoring to track replication performance and health."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create monitoring view for replication status\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE VIEW {DA.catalog}.{DA.schema}.replication_status AS\n",
        "SELECT \n",
        "    '{DA.source_table}' as source_table,\n",
        "    '{DA.replica_table}' as replica_table,\n",
        "    '{r2_table_path}' as r2_location,\n",
        "    source.record_count as source_records,\n",
        "    replica.record_count as replica_records,\n",
        "    source.latest_version as source_version,\n",
        "    replica.last_processed_version,\n",
        "    CASE \n",
        "        WHEN source.record_count = replica.record_count THEN 'IN_SYNC'\n",
        "        ELSE 'OUT_OF_SYNC'\n",
        "    END as sync_status,\n",
        "    current_timestamp() as check_time\n",
        "FROM (\n",
        "    SELECT \n",
        "        COUNT(*) as record_count,\n",
        "        MAX(version) as latest_version\n",
        "    FROM (\n",
        "        SELECT version FROM (\n",
        "            DESCRIBE DETAIL {DA.catalog}.{DA.schema}.{DA.source_table}\n",
        "        )\n",
        "    ) v\n",
        "    CROSS JOIN (\n",
        "        SELECT COUNT(*) as cnt FROM {DA.catalog}.{DA.schema}.{DA.source_table}\n",
        "    ) c\n",
        ") source\n",
        "CROSS JOIN (\n",
        "    SELECT \n",
        "        COUNT(*) as record_count,\n",
        "        COALESCE(props.last_processed_version, '0') as last_processed_version\n",
        "    FROM (\n",
        "        SELECT COUNT(*) as cnt FROM {DA.catalog}.{DA.schema}.{DA.replica_table}\n",
        "    ) c\n",
        "    CROSS JOIN (\n",
        "        SELECT properties['last_processed_version'] as last_processed_version\n",
        "        FROM (\n",
        "            DESCRIBE DETAIL {DA.catalog}.{DA.schema}.{DA.replica_table}\n",
        "        )\n",
        "    ) props\n",
        ") replica\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Created replication monitoring view\")\n",
        "\n",
        "# Check current status\n",
        "print(\"\\nüìä Current Replication Status:\")\n",
        "display(spark.sql(f\"SELECT * FROM {DA.catalog}.{DA.schema}.replication_status\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "üéâ **Congratulations!** You have successfully set up cross-cloud replication with Cloudflare R2.\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "‚úÖ **Source Setup**: Created a managed table with Change Data Feed enabled  \n",
        "‚úÖ **R2 Configuration**: Set up Cloudflare R2 external storage with S3-compatible API  \n",
        "‚úÖ **Initial Replication**: Performed full data copy to R2 external table  \n",
        "‚úÖ **Change Detection**: Used CDF to detect and capture data changes  \n",
        "‚úÖ **Incremental Updates**: Applied changes to R2 replica automatically  \n",
        "‚úÖ **Production Function**: Created job-ready replication function  \n",
        "‚úÖ **Monitoring**: Set up observability for replication status  \n",
        "\n",
        "### Key Benefits Achieved:\n",
        "\n",
        "üåç **Global Distribution**: Data available worldwide via Cloudflare's network  \n",
        "üí∞ **Zero Egress Costs**: Eliminate expensive data transfer fees  \n",
        "‚ö° **High Performance**: Fast access through global CDN  \n",
        "üîÑ **Automated Sync**: Real-time change propagation using CDF  \n",
        "üìä **Monitoring**: Full observability into replication health  \n",
        "\n",
        "### Next Steps for Production:\n",
        "\n",
        "1. **Schedule the Job**: Set up the replication function as a Databricks Job (hourly/daily)\n",
        "2. **Add Secrets**: Store R2 credentials in Databricks Secrets\n",
        "3. **Error Handling**: Implement retry logic and alerting\n",
        "4. **Performance Tuning**: Optimize for your data volume and frequency\n",
        "5. **Security**: Configure appropriate access controls and encryption\n",
        "6. **Monitoring**: Set up dashboards and alerts for replication health\n",
        "\n",
        "### Scheduling as a Job:\n",
        "\n",
        "To schedule this as a Databricks Job:\n",
        "\n",
        "1. Create a new job in Databricks Workflows\n",
        "2. Use this notebook or create a dedicated script\n",
        "3. Set appropriate cluster configuration\n",
        "4. Configure schedule (e.g., every 15 minutes)\n",
        "5. Add email alerts for failures\n",
        "\n",
        "The recipients can now access this data through Delta Sharing with zero egress costs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "3.1 DEMO Cross Cloud Replication with Cloudflare R2 [Provider]",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
