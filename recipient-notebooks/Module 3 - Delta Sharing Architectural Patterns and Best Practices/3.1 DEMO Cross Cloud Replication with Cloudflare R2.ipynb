{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img\n",
        "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
        "    alt=\"Databricks Learning\"\n",
        "  >\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.1 DEMO: Cross Cloud Replication with Cloudflare R2 [Recipient]\n",
        "\n",
        "## Overview\n",
        "This demo showcases how recipients can access replicated data from Cloudflare R2 and maintain synchronized local copies using `MERGE` operations for Type 1 Slowly Changing Dimensions (SCD). Recipients read changes from the R2-hosted external table and apply them to their local tables.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this demo, you will understand:\n",
        "1. How to access external tables hosted on Cloudflare R2\n",
        "2. How to detect changes in replicated data\n",
        "3. How to implement Type 1 SCD using `MERGE` operations\n",
        "4. How to schedule automatic synchronization\n",
        "\n",
        "We will use a Cloudflare R2 bucket as the storage location for an External Table which is used between the provider and recipients to store changes from a Managed source table, the changes are then replication asynchronously to another Managed table in the recipients region/provider.\n",
        "<br />\n",
        "<br />\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img\n",
        "    src=\"https://github.com/stackql/databricks-data-sharing-and-collaboration/blob/main/images/cloudflare-r2-replication.png?raw=true\"\n",
        "    alt=\"Cloudflare R2 Replication\"\n",
        "  >\n",
        "</div>\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "**Benefits:**\n",
        "- Zero egress costs with Cloudflare R2\n",
        "- Global data distribution without provider dependencies\n",
        "- Automated change propagation using CDF\n",
        "- Cost-effective sharing with unlimited recipients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run the common setup and demo configuration scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./Demo-Setup-3_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create Storage Credential for R2 Access\n",
        "\n",
        "Create storage credential to access the Cloudflare R2 bucket (read-only access)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE STORAGE CREDENTIAL IF NOT EXISTS r2_credential\n",
        "WITH (\n",
        "  AWS_ACCESS_KEY_ID 'your-r2-access-key',\n",
        "  AWS_SECRET_ACCESS_KEY 'your-r2-secret-key'\n",
        ")\n",
        "COMMENT 'Cloudflare R2 credentials for reading replicated data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create External Location for R2\n",
        "\n",
        "Create external location pointing to the R2 bucket where provider stores data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE EXTERNAL LOCATION IF NOT EXISTS r2_location\n",
        "URL 's3://databricks-demo/'\n",
        "WITH (\n",
        "  STORAGE_CREDENTIAL r2_credential\n",
        ")\n",
        "COMMENT 'Cloudflare R2 bucket for accessing replicated data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create External Table Reference to R2 Data\n",
        "\n",
        "Create an external table that points to the provider's data on R2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE OR REPLACE TABLE IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data')) (\n",
        "  transaction_id STRING,\n",
        "  customer_id STRING,\n",
        "  product_category STRING,\n",
        "  amount DECIMAL(10,2),\n",
        "  transaction_date DATE,\n",
        "  region STRING,\n",
        "  created_at TIMESTAMP\n",
        ")\n",
        "LOCATION CONCAT('s3://databricks-demo/', DA.r2_path)\n",
        "COMMENT 'External table pointing to provider data on Cloudflare R2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify R2 Data Access\n",
        "\n",
        "Check that we can successfully access the data from R2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Check data availability from R2\n",
        "SELECT \n",
        "  COUNT(*) as total_records,\n",
        "  MIN(transaction_date) as earliest_date,\n",
        "  MAX(transaction_date) as latest_date,\n",
        "  SUM(amount) as total_amount\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Sample data from R2\n",
        "SELECT * FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))\n",
        "ORDER BY created_at\n",
        "LIMIT 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Local Managed Table (Type 1 SCD)\n",
        "\n",
        "Create a local managed table to store synchronized data with SCD metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE OR REPLACE TABLE IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions')) (\n",
        "  transaction_id STRING,\n",
        "  customer_id STRING,\n",
        "  product_category STRING,\n",
        "  amount DECIMAL(10,2),\n",
        "  transaction_date DATE,\n",
        "  region STRING,\n",
        "  created_at TIMESTAMP,\n",
        "  -- Type 1 SCD metadata\n",
        "  last_updated_at TIMESTAMP,\n",
        "  sync_timestamp TIMESTAMP\n",
        ")\n",
        "PARTITIONED BY (transaction_date)\n",
        "TBLPROPERTIES (\n",
        "  'delta.autoOptimize.optimizeWrite' = 'true',\n",
        "  'delta.enableChangeDataFeed' = 'true'\n",
        ")\n",
        "COMMENT 'Local synchronized table with Type 1 SCD metadata'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initial Data Load Using `MERGE`\n",
        "\n",
        "Perform initial load from R2 to local table using `MERGE` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MERGE INTO IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions')) AS target\n",
        "USING (\n",
        "  SELECT \n",
        "    transaction_id,\n",
        "    customer_id,\n",
        "    product_category,\n",
        "    amount,\n",
        "    transaction_date,\n",
        "    region,\n",
        "    created_at,\n",
        "    current_timestamp() as last_updated_at,\n",
        "    current_timestamp() as sync_timestamp\n",
        "  FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))\n",
        ") AS source\n",
        "ON target.transaction_id = source.transaction_id\n",
        "\n",
        "WHEN MATCHED THEN UPDATE SET\n",
        "  customer_id = source.customer_id,\n",
        "  product_category = source.product_category,\n",
        "  amount = source.amount,\n",
        "  transaction_date = source.transaction_date,\n",
        "  region = source.region,\n",
        "  created_at = source.created_at,\n",
        "  last_updated_at = source.last_updated_at,\n",
        "  sync_timestamp = source.sync_timestamp\n",
        "\n",
        "WHEN NOT MATCHED THEN INSERT (\n",
        "  transaction_id, customer_id, product_category, amount,\n",
        "  transaction_date, region, created_at, last_updated_at, sync_timestamp\n",
        ") VALUES (\n",
        "  source.transaction_id, source.customer_id, source.product_category,\n",
        "  source.amount, source.transaction_date, source.region,\n",
        "  source.created_at, source.last_updated_at, source.sync_timestamp\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verify Initial Synchronization\n",
        "\n",
        "Check that the initial sync was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Compare record counts\n",
        "SELECT \n",
        "  'R2 Source' as source,\n",
        "  COUNT(*) as record_count,\n",
        "  SUM(amount) as total_amount\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "  'Local Sync' as source,\n",
        "  COUNT(*) as record_count,\n",
        "  SUM(amount) as total_amount\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- View synchronized data with SCD metadata\n",
        "SELECT \n",
        "  transaction_id,\n",
        "  amount,\n",
        "  region,\n",
        "  transaction_date,\n",
        "  last_updated_at,\n",
        "  sync_timestamp\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions'))\n",
        "ORDER BY sync_timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Simulate Provider Adding New Data\n",
        "\n",
        "Wait for the provider to add new data to R2, then refresh our external table to see changes.\n",
        "\n",
        "**Note**: In a real scenario, you would monitor for changes or run this on a schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Refresh external table to pick up any new data from R2\n",
        "REFRESH TABLE IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Check for new data\n",
        "SELECT COUNT(*) as current_r2_count \n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Detect and Sync Changes\n",
        "\n",
        "Identify changes between R2 source and local table, then sync using `MERGE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Identify new/changed records\n",
        "SELECT \n",
        "  'New Records' as change_type,\n",
        "  COUNT(*) as count\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data')) r2\n",
        "LEFT ANTI JOIN IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions')) local\n",
        "ON r2.transaction_id = local.transaction_id\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "  'Updated Records' as change_type,\n",
        "  COUNT(*) as count\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data')) r2\n",
        "INNER JOIN IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions')) local\n",
        "ON r2.transaction_id = local.transaction_id\n",
        "WHERE r2.amount != local.amount \n",
        "   OR r2.region != local.region \n",
        "   OR r2.product_category != local.product_category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Apply Changes Using `MERGE` (Type 1 SCD)\n",
        "\n",
        "Synchronize any detected changes using `MERGE` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MERGE INTO IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions')) AS target\n",
        "USING (\n",
        "  SELECT \n",
        "    transaction_id,\n",
        "    customer_id,\n",
        "    product_category,\n",
        "    amount,\n",
        "    transaction_date,\n",
        "    region,\n",
        "    created_at,\n",
        "    current_timestamp() as last_updated_at,\n",
        "    current_timestamp() as sync_timestamp\n",
        "  FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))\n",
        ") AS source\n",
        "ON target.transaction_id = source.transaction_id\n",
        "\n",
        "WHEN MATCHED THEN UPDATE SET\n",
        "  customer_id = source.customer_id,\n",
        "  product_category = source.product_category,\n",
        "  amount = source.amount,\n",
        "  transaction_date = source.transaction_date,\n",
        "  region = source.region,\n",
        "  created_at = source.created_at,\n",
        "  last_updated_at = source.last_updated_at,\n",
        "  sync_timestamp = source.sync_timestamp\n",
        "\n",
        "WHEN NOT MATCHED THEN INSERT (\n",
        "  transaction_id, customer_id, product_category, amount,\n",
        "  transaction_date, region, created_at, last_updated_at, sync_timestamp\n",
        ") VALUES (\n",
        "  source.transaction_id, source.customer_id, source.product_category,\n",
        "  source.amount, source.transaction_date, source.region,\n",
        "  source.created_at, source.last_updated_at, source.sync_timestamp\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Final Verification\n",
        "\n",
        "Verify that synchronization is complete and data quality is maintained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Final sync verification\n",
        "SELECT \n",
        "  'R2 Source' as table_name,\n",
        "  COUNT(*) as total_records,\n",
        "  SUM(amount) as total_amount,\n",
        "  MAX(created_at) as latest_record\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "  'Local Synchronized' as table_name,\n",
        "  COUNT(*) as total_records,\n",
        "  SUM(amount) as total_amount,\n",
        "  MAX(created_at) as latest_record\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- View Type 1 SCD metadata\n",
        "SELECT \n",
        "  transaction_id,\n",
        "  amount,\n",
        "  region,\n",
        "  transaction_date,\n",
        "  last_updated_at,\n",
        "  sync_timestamp,\n",
        "  CASE \n",
        "    WHEN last_updated_at > DATE_SUB(current_timestamp(), 1) THEN 'Recently Updated'\n",
        "    ELSE 'Older Record'\n",
        "  END as freshness_status\n",
        "FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions'))\n",
        "ORDER BY last_updated_at DESC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Create Reusable Sync View\n",
        "\n",
        "Create a view that can be used to easily check sync status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE OR REPLACE VIEW IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.sync_status')) AS\n",
        "SELECT \n",
        "  r2.record_count as r2_records,\n",
        "  local.record_count as local_records,\n",
        "  CASE \n",
        "    WHEN r2.record_count = local.record_count THEN 'IN_SYNC'\n",
        "    ELSE 'OUT_OF_SYNC'\n",
        "  END as sync_status,\n",
        "  r2.total_amount as r2_total_amount,\n",
        "  local.total_amount as local_total_amount,\n",
        "  local.last_sync,\n",
        "  current_timestamp() as check_time\n",
        "FROM (\n",
        "  SELECT \n",
        "    COUNT(*) as record_count,\n",
        "    SUM(amount) as total_amount\n",
        "  FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.r2_source_data'))\n",
        ") r2\n",
        "CROSS JOIN (\n",
        "  SELECT \n",
        "    COUNT(*) as record_count,\n",
        "    SUM(amount) as total_amount,\n",
        "    MAX(sync_timestamp) as last_sync\n",
        "  FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.local_transactions'))\n",
        ") local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-- Check current sync status\n",
        "SELECT * FROM IDENTIFIER(CONCAT(DA.catalog, '.', DA.schema, '.sync_status'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "✅ **R2 Access**: Connected to Cloudflare R2 external table with zero egress costs  \n",
        "✅ **Local Table**: Created managed table with Type 1 SCD metadata  \n",
        "✅ **Change Detection**: Identified new and updated records from R2 source  \n",
        "✅ **MERGE Operations**: Used `MERGE` for efficient Type 1 SCD synchronization  \n",
        "✅ **Data Quality**: Verified synchronization accuracy and completeness  \n",
        "✅ **Monitoring**: Created sync status view for ongoing monitoring  \n",
        "\n",
        "### Type 1 SCD Implementation:\n",
        "\n",
        "Our Type 1 SCD approach provides:\n",
        "- **Current Data Only**: Overwrites old values with new ones (no history)\n",
        "- **Metadata Tracking**: `last_updated_at` and `sync_timestamp` for auditing\n",
        "- **`MERGE` Logic**: Efficient upsert operations (`UPDATE` existing, `INSERT` new)\n",
        "- **Change Detection**: Automatic identification of modifications\n",
        "\n",
        "### Key Benefits:\n",
        "\n",
        "**Global Access**: Fast data access from Cloudflare's global network  \n",
        "**Cost Efficient**: Zero egress fees for reading from R2  \n",
        "**Local Performance**: Optimized local queries on synchronized data  \n",
        "**Automatic Updates**: MERGE-based synchronization maintains current data  \n",
        "**Data Quality**: Built-in validation and monitoring capabilities  \n",
        "\n",
        "### Next Steps for Production:\n",
        "\n",
        "1. **Automation**: Schedule the `MERGE` operation as a Databricks Job (hourly/daily)\n",
        "2. **Secrets**: Use Databricks Secrets for R2 credentials\n",
        "3. **Monitoring**: Set up alerts for sync failures or data quality issues\n",
        "4. **Optimization**: Add partition pruning and table optimization\n",
        "5. **Advanced SCD**: Consider Type 2 SCD if historical tracking is needed\n",
        "\n",
        "The recipient now has a robust, cost-effective way to stay synchronized with provider data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "language": "sql",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "3.1 DEMO Cross Cloud Replication with Cloudflare R2 [Recipient]",
      "widgets": {}
    },
    "language_info": {
      "name": "sql"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
