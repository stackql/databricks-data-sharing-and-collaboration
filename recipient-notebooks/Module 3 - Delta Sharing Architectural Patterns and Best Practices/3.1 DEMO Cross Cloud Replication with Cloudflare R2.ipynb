{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img\n",
        "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
        "    alt=\"Databricks Learning\"\n",
        "  >\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.1 DEMO: Cross Cloud Replication with Cloudflare R2 [Recipient]\n",
        "\n",
        "## Overview\n",
        "This demo showcases how recipients can access replicated data from Cloudflare R2 and maintain local synchronized copies using MERGE operations for Type 1 Slowly Changing Dimensions (SCD). Recipients read changes from the R2-hosted external table and apply them to their local tables.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this demo, you will understand:\n",
        "1. How to access external tables hosted on Cloudflare R2\n",
        "2. How to detect changes in replicated data\n",
        "3. How to implement Type 1 SCD using MERGE operations\n",
        "4. How to schedule automatic synchronization jobs\n",
        "\n",
        "## Architecture\n",
        "```\n",
        "Provider R2 External Table → Change Detection → Local Table (MERGE) → Type 1 SCD\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Access globally distributed data with zero egress costs\n",
        "- Maintain local copies for optimal query performance\n",
        "- Automatic change detection and synchronization\n",
        "- Type 1 SCD pattern for historical data management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run the common setup and demo configuration scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./Demo-Setup-3_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configure Cloudflare R2 Access\n",
        "\n",
        "Configure access to the Cloudflare R2 bucket where the provider has replicated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# R2 Configuration - Same as provider (read-only access)\n",
        "r2_endpoint = \"https://4132d7d5587ee99b9d482ecfc2c1853c.r2.cloudflarestorage.com\"\n",
        "r2_bucket = \"databricks-demo\"\n",
        "r2_access_key = \"<your-r2-access-key>\"  # Replace with actual key\n",
        "r2_secret_key = \"<your-r2-secret-key>\"  # Replace with actual key\n",
        "\n",
        "# Path to the replicated data\n",
        "r2_table_path = f\"s3a://{r2_bucket}/{DA.r2_path}\"\n",
        "\n",
        "# Local table for synchronized copy\n",
        "local_table_name = f\"{DA.catalog}.{DA.schema}.local_transactions\"\n",
        "\n",
        "print(f\"R2 Source Path: {r2_table_path}\")\n",
        "print(f\"Local Table: {local_table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Spark for R2 access\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.endpoint\", r2_endpoint)\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.access.key\", r2_access_key)\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.secret.key\", r2_secret_key)\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.path.style.access\", \"true\")\n",
        "spark.conf.set(f\"fs.s3a.bucket.{r2_bucket}.connection.ssl.enabled\", \"true\")\n",
        "\n",
        "print(\"✅ Spark configured for R2 access\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Discover and Access R2 External Table\n",
        "\n",
        "Access the external table hosted on Cloudflare R2 by the provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create external table reference to R2 data\n",
        "# Note: This assumes the provider has shared the table structure and location\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {DA.catalog}.{DA.schema}.r2_source_data (\n",
        "  transaction_id STRING,\n",
        "  customer_id STRING,\n",
        "  product_category STRING,\n",
        "  amount DECIMAL(10,2),\n",
        "  transaction_date DATE,\n",
        "  region STRING,\n",
        "  created_at TIMESTAMP\n",
        ")\n",
        "USING DELTA\n",
        "LOCATION '{r2_table_path}'\n",
        "\"\"\")\n",
        "\n",
        "print(f\"✅ Created external table reference: {DA.catalog}.{DA.schema}.r2_source_data\")\n",
        "print(f\"✅ Location: {r2_table_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify we can access the R2 data\n",
        "r2_count = spark.table(f\"{DA.catalog}.{DA.schema}.r2_source_data\").count()\n",
        "print(f\"📊 Found {r2_count} records in R2 external table\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\n📄 Sample data from R2:\")\n",
        "display(spark.table(f\"{DA.catalog}.{DA.schema}.r2_source_data\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Local Target Table\n",
        "\n",
        "Create a local managed table that will store our synchronized copy of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create local target table with additional SCD metadata\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {local_table_name} (\n",
        "  transaction_id STRING,\n",
        "  customer_id STRING,\n",
        "  product_category STRING,\n",
        "  amount DECIMAL(10,2),\n",
        "  transaction_date DATE,\n",
        "  region STRING,\n",
        "  created_at TIMESTAMP,\n",
        "  -- SCD Type 1 metadata\n",
        "  last_updated_at TIMESTAMP,\n",
        "  source_version BIGINT,\n",
        "  sync_timestamp TIMESTAMP\n",
        ")\n",
        "USING DELTA\n",
        "PARTITIONED BY (transaction_date)\n",
        "TBLPROPERTIES (\n",
        "  'delta.autoOptimize.optimizeWrite' = 'true',\n",
        "  'delta.autoOptimize.autoCompact' = 'true',\n",
        "  'delta.enableChangeDataFeed' = 'true'\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "print(f\"✅ Created local target table: {local_table_name}\")\n",
        "print(\"✅ Table includes SCD Type 1 metadata columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Initial Data Load\n",
        "\n",
        "Perform the initial load of data from R2 to our local table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current version of R2 source\n",
        "r2_version = spark.sql(f\"DESCRIBE DETAIL {DA.catalog}.{DA.schema}.r2_source_data\").select(\"version\").collect()[0][0]\n",
        "sync_time = datetime.now()\n",
        "\n",
        "print(f\"📊 R2 source version: {r2_version}\")\n",
        "print(f\"📊 Sync timestamp: {sync_time}\")\n",
        "\n",
        "# Initial load with SCD metadata\n",
        "initial_data = spark.sql(f\"\"\"\n",
        "SELECT \n",
        "  transaction_id,\n",
        "  customer_id,\n",
        "  product_category,\n",
        "  amount,\n",
        "  transaction_date,\n",
        "  region,\n",
        "  created_at,\n",
        "  -- SCD Type 1 metadata\n",
        "  created_at as last_updated_at,\n",
        "  {r2_version} as source_version,\n",
        "  '{sync_time}' as sync_timestamp\n",
        "FROM {DA.catalog}.{DA.schema}.r2_source_data\n",
        "\"\"\")\n",
        "\n",
        "initial_data.write.mode(\"overwrite\").saveAsTable(local_table_name)\n",
        "\n",
        "loaded_count = spark.table(local_table_name).count()\n",
        "print(f\"✅ Initial load completed: {loaded_count} records synchronized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Change Detection Function\n",
        "\n",
        "Create a function to detect changes in the R2 source data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_r2_changes(r2_table, local_table):\n",
        "    \"\"\"\n",
        "    Detect changes between R2 source and local table.\n",
        "    \n",
        "    Args:\n",
        "        r2_table: Name of R2 external table\n",
        "        local_table: Name of local synchronized table\n",
        "    \n",
        "    Returns:\n",
        "        dict: Information about detected changes\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get current versions and record counts\n",
        "        r2_version = spark.sql(f\"DESCRIBE DETAIL {r2_table}\").select(\"version\").collect()[0][0]\n",
        "        r2_count = spark.table(r2_table).count()\n",
        "        \n",
        "        local_version_result = spark.sql(f\"SELECT MAX(source_version) as max_version FROM {local_table}\").collect()\n",
        "        local_version = local_version_result[0][0] if local_version_result[0][0] is not None else 0\n",
        "        local_count = spark.table(local_table).count()\n",
        "        \n",
        "        # Detect if there are changes\n",
        "        has_changes = (r2_version > local_version) or (r2_count != local_count)\n",
        "        \n",
        "        # If there are changes, identify new/updated records\n",
        "        new_records = 0\n",
        "        updated_records = 0\n",
        "        \n",
        "        if has_changes:\n",
        "            # Find records that exist in R2 but not in local (new records)\n",
        "            new_records_df = spark.sql(f\"\"\"\n",
        "                SELECT r2.*\n",
        "                FROM {r2_table} r2\n",
        "                LEFT ANTI JOIN {local_table} local\n",
        "                ON r2.transaction_id = local.transaction_id\n",
        "            \"\"\")\n",
        "            new_records = new_records_df.count()\n",
        "            \n",
        "            # Find records that exist in both but have different values (updated records)\n",
        "            updated_records_df = spark.sql(f\"\"\"\n",
        "                SELECT r2.*\n",
        "                FROM {r2_table} r2\n",
        "                INNER JOIN {local_table} local\n",
        "                ON r2.transaction_id = local.transaction_id\n",
        "                WHERE r2.amount != local.amount \n",
        "                   OR r2.product_category != local.product_category\n",
        "                   OR r2.region != local.region\n",
        "                   OR r2.created_at != local.created_at\n",
        "            \"\"\")\n",
        "            updated_records = updated_records_df.count()\n",
        "        \n",
        "        return {\n",
        "            'has_changes': has_changes,\n",
        "            'r2_version': r2_version,\n",
        "            'local_version': local_version,\n",
        "            'r2_count': r2_count,\n",
        "            'local_count': local_count,\n",
        "            'new_records': new_records,\n",
        "            'updated_records': updated_records,\n",
        "            'total_changes': new_records + updated_records\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'has_changes': False,\n",
        "            'error': str(e),\n",
        "            'message': f'Error detecting changes: {str(e)}'\n",
        "        }\n",
        "\n",
        "print(\"✅ Change detection function created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Create MERGE-based Synchronization Function\n",
        "\n",
        "Create a function that uses MERGE operations to synchronize changes from R2 to local table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sync_r2_to_local(r2_table, local_table):\n",
        "    \"\"\"\n",
        "    Synchronize data from R2 to local table using MERGE (Type 1 SCD).\n",
        "    \n",
        "    Args:\n",
        "        r2_table: Name of R2 external table\n",
        "        local_table: Name of local target table\n",
        "    \n",
        "    Returns:\n",
        "        dict: Synchronization results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sync_start = datetime.now()\n",
        "        \n",
        "        # Detect changes first\n",
        "        change_info = detect_r2_changes(r2_table, local_table)\n",
        "        \n",
        "        if not change_info.get('has_changes', False):\n",
        "            return {\n",
        "                'status': 'up_to_date',\n",
        "                'records_processed': 0,\n",
        "                'message': 'No changes detected - local table is up to date'\n",
        "            }\n",
        "        \n",
        "        print(f\"🔄 Synchronizing changes...\")\n",
        "        print(f\"   New records: {change_info['new_records']}\")\n",
        "        print(f\"   Updated records: {change_info['updated_records']}\")\n",
        "        \n",
        "        # Get current R2 version for metadata\n",
        "        r2_version = change_info['r2_version']\n",
        "        \n",
        "        # Perform MERGE operation (Type 1 SCD)\n",
        "        merge_sql = f\"\"\"\n",
        "        MERGE INTO {local_table} AS target\n",
        "        USING (\n",
        "            SELECT \n",
        "                transaction_id,\n",
        "                customer_id,\n",
        "                product_category,\n",
        "                amount,\n",
        "                transaction_date,\n",
        "                region,\n",
        "                created_at,\n",
        "                -- SCD metadata\n",
        "                current_timestamp() as last_updated_at,\n",
        "                {r2_version} as source_version,\n",
        "                current_timestamp() as sync_timestamp\n",
        "            FROM {r2_table}\n",
        "        ) AS source\n",
        "        ON target.transaction_id = source.transaction_id\n",
        "        \n",
        "        WHEN MATCHED THEN UPDATE SET\n",
        "            customer_id = source.customer_id,\n",
        "            product_category = source.product_category,\n",
        "            amount = source.amount,\n",
        "            transaction_date = source.transaction_date,\n",
        "            region = source.region,\n",
        "            created_at = source.created_at,\n",
        "            last_updated_at = source.last_updated_at,\n",
        "            source_version = source.source_version,\n",
        "            sync_timestamp = source.sync_timestamp\n",
        "        \n",
        "        WHEN NOT MATCHED THEN INSERT (\n",
        "            transaction_id, customer_id, product_category, amount,\n",
        "            transaction_date, region, created_at, last_updated_at,\n",
        "            source_version, sync_timestamp\n",
        "        ) VALUES (\n",
        "            source.transaction_id, source.customer_id, source.product_category,\n",
        "            source.amount, source.transaction_date, source.region,\n",
        "            source.created_at, source.last_updated_at, source.source_version,\n",
        "            source.sync_timestamp\n",
        "        )\n",
        "        \"\"\"\n",
        "        \n",
        "        # Execute the MERGE\n",
        "        merge_result = spark.sql(merge_sql)\n",
        "        \n",
        "        sync_end = datetime.now()\n",
        "        duration = (sync_end - sync_start).total_seconds()\n",
        "        \n",
        "        # Get final counts\n",
        "        final_local_count = spark.table(local_table).count()\n",
        "        \n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'records_processed': change_info['total_changes'],\n",
        "            'new_records': change_info['new_records'],\n",
        "            'updated_records': change_info['updated_records'],\n",
        "            'final_count': final_local_count,\n",
        "            'duration_seconds': duration,\n",
        "            'r2_version': r2_version,\n",
        "            'sync_timestamp': sync_start.isoformat(),\n",
        "            'message': f'Successfully synchronized {change_info[\"total_changes\"]} changes'\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'records_processed': 0,\n",
        "            'error': str(e),\n",
        "            'message': f'Synchronization failed: {str(e)}'\n",
        "        }\n",
        "\n",
        "print(\"✅ MERGE-based synchronization function created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test Change Detection\n",
        "\n",
        "Test our change detection capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test change detection\n",
        "change_info = detect_r2_changes(\n",
        "    f\"{DA.catalog}.{DA.schema}.r2_source_data\",\n",
        "    local_table_name\n",
        ")\n",
        "\n",
        "print(\"📊 Change Detection Results:\")\n",
        "print(f\"   Has changes: {change_info.get('has_changes', 'Unknown')}\")\n",
        "print(f\"   R2 version: {change_info.get('r2_version', 'Unknown')}\")\n",
        "print(f\"   Local version: {change_info.get('local_version', 'Unknown')}\")\n",
        "print(f\"   R2 record count: {change_info.get('r2_count', 'Unknown')}\")\n",
        "print(f\"   Local record count: {change_info.get('local_count', 'Unknown')}\")\n",
        "print(f\"   New records: {change_info.get('new_records', 'Unknown')}\")\n",
        "print(f\"   Updated records: {change_info.get('updated_records', 'Unknown')}\")\n",
        "\n",
        "if change_info.get('error'):\n",
        "    print(f\"   Error: {change_info['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Simulate Provider Adding New Data\n",
        "\n",
        "For demo purposes, let's simulate the provider adding new data to R2. \n",
        "In practice, this would happen automatically through the provider's replication process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate new data being added to R2 by the provider\n",
        "# In real scenarios, this would come from the provider's replication job\n",
        "\n",
        "import uuid\n",
        "from datetime import date\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, DateType, TimestampType\n",
        "\n",
        "# Create some new simulated data\n",
        "new_provider_data = []\n",
        "regions = ['North America', 'Europe', 'Asia Pacific', 'Latin America']\n",
        "categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports']\n",
        "\n",
        "for i in range(300, 325):\n",
        "    new_provider_data.append((\n",
        "        str(uuid.uuid4()),\n",
        "        f\"customer_{i % 80}\",\n",
        "        categories[i % len(categories)],\n",
        "        round(120 + (i * 3.1) % 350, 2),\n",
        "        date(2024, 10, 27),  # Today's date\n",
        "        regions[i % len(regions)],\n",
        "        datetime.now()\n",
        "    ))\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"product_category\", StringType(), True),\n",
        "    StructField(\"amount\", DecimalType(10,2), True),\n",
        "    StructField(\"transaction_date\", DateType(), True),\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"created_at\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "new_df = spark.createDataFrame(new_provider_data, schema)\n",
        "\n",
        "# Add to the R2 table (simulating provider replication)\n",
        "new_df.write.mode(\"append\").saveAsTable(f\"{DA.catalog}.{DA.schema}.r2_source_data\")\n",
        "\n",
        "print(f\"🎭 Simulated provider adding {new_df.count()} new records to R2\")\n",
        "\n",
        "# Check the new state\n",
        "updated_r2_count = spark.table(f\"{DA.catalog}.{DA.schema}.r2_source_data\").count()\n",
        "print(f\"📊 R2 table now has {updated_r2_count} total records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Synchronization with MERGE\n",
        "\n",
        "Now test our synchronization function to pull the new changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run synchronization\n",
        "sync_result = sync_r2_to_local(\n",
        "    f\"{DA.catalog}.{DA.schema}.r2_source_data\",\n",
        "    local_table_name\n",
        ")\n",
        "\n",
        "print(\"🔄 Synchronization Results:\")\n",
        "print(f\"   Status: {sync_result['status']}\")\n",
        "print(f\"   Records processed: {sync_result.get('records_processed', 0)}\")\n",
        "print(f\"   New records: {sync_result.get('new_records', 0)}\")\n",
        "print(f\"   Updated records: {sync_result.get('updated_records', 0)}\")\n",
        "print(f\"   Duration: {sync_result.get('duration_seconds', 0):.2f} seconds\")\n",
        "print(f\"   Final count: {sync_result.get('final_count', 0)}\")\n",
        "print(f\"   Message: {sync_result.get('message', 'No message')}\")\n",
        "\n",
        "if sync_result.get('error'):\n",
        "    print(f\"   Error: {sync_result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Verify Type 1 SCD Implementation\n",
        "\n",
        "Verify that our Type 1 SCD implementation is working correctly with proper metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the SCD metadata in our local table\n",
        "scd_summary = spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    COUNT(*) as total_records,\n",
        "    COUNT(DISTINCT source_version) as distinct_versions,\n",
        "    MIN(sync_timestamp) as first_sync,\n",
        "    MAX(sync_timestamp) as last_sync,\n",
        "    MAX(source_version) as latest_version,\n",
        "    COUNT(DISTINCT transaction_date) as distinct_dates\n",
        "FROM {local_table_name}\n",
        "\"\"\")\n",
        "\n",
        "print(\"📊 Type 1 SCD Summary:\")\n",
        "display(scd_summary)\n",
        "\n",
        "# Show sample records with SCD metadata\n",
        "print(\"\\n📄 Sample records with SCD metadata:\")\n",
        "sample_scd = spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    transaction_id,\n",
        "    amount,\n",
        "    region,\n",
        "    transaction_date,\n",
        "    last_updated_at,\n",
        "    source_version,\n",
        "    sync_timestamp\n",
        "FROM {local_table_name}\n",
        "ORDER BY sync_timestamp DESC\n",
        "LIMIT 10\n",
        "\"\"\")\n",
        "display(sample_scd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Test Update Scenarios\n",
        "\n",
        "Test how our MERGE logic handles updates to existing records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate the provider updating some existing records in R2\n",
        "# Get a few transaction IDs to update\n",
        "sample_ids = spark.sql(f\"\"\"\n",
        "    SELECT transaction_id \n",
        "    FROM {local_table_name} \n",
        "    LIMIT 5\n",
        "\"\"\").collect()\n",
        "\n",
        "if len(sample_ids) > 0:\n",
        "    # Update these records in the R2 table (simulate provider changes)\n",
        "    for row in sample_ids[:3]:  # Update first 3 records\n",
        "        transaction_id = row[0]\n",
        "        # Increase amount by 10% and change region\n",
        "        spark.sql(f\"\"\"\n",
        "            UPDATE {DA.catalog}.{DA.schema}.r2_source_data\n",
        "            SET amount = amount * 1.1,\n",
        "                region = 'Updated Region',\n",
        "                created_at = current_timestamp()\n",
        "            WHERE transaction_id = '{transaction_id}'\n",
        "        \"\"\")\n",
        "    \n",
        "    print(f\"🔄 Simulated provider updating 3 existing records in R2\")\n",
        "    \n",
        "    # Now synchronize the updates\n",
        "    update_sync_result = sync_r2_to_local(\n",
        "        f\"{DA.catalog}.{DA.schema}.r2_source_data\",\n",
        "        local_table_name\n",
        "    )\n",
        "    \n",
        "    print(\"\\n📊 Update Synchronization Results:\")\n",
        "    print(f\"   Status: {update_sync_result['status']}\")\n",
        "    print(f\"   Records processed: {update_sync_result.get('records_processed', 0)}\")\n",
        "    print(f\"   Updated records: {update_sync_result.get('updated_records', 0)}\")\n",
        "    print(f\"   Message: {update_sync_result.get('message', 'No message')}\")\n",
        "    \n",
        "    # Show the updated records\n",
        "    print(\"\\n📄 Updated records (Type 1 SCD - current values only):\")\n",
        "    updated_records = spark.sql(f\"\"\"\n",
        "        SELECT \n",
        "            transaction_id,\n",
        "            amount,\n",
        "            region,\n",
        "            last_updated_at,\n",
        "            source_version\n",
        "        FROM {local_table_name}\n",
        "        WHERE region = 'Updated Region'\n",
        "        ORDER BY last_updated_at DESC\n",
        "    \"\"\")\n",
        "    display(updated_records)\n",
        "else:\n",
        "    print(\"ℹ️  No records found to update\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Create Production Synchronization Job\n",
        "\n",
        "Create a production-ready job function that can be scheduled to run automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def production_sync_job():\n",
        "    \"\"\"\n",
        "    Production-ready synchronization job for recipient.\n",
        "    This function includes comprehensive error handling, logging, and monitoring.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    \n",
        "    job_start = datetime.now()\n",
        "    \n",
        "    try:\n",
        "        print(f\"🔄 Starting recipient sync job at {job_start}\")\n",
        "        \n",
        "        # Define table names\n",
        "        r2_source = f\"{DA.catalog}.{DA.schema}.r2_source_data\"\n",
        "        local_target = local_table_name\n",
        "        \n",
        "        # Check if R2 source is accessible\n",
        "        try:\n",
        "            r2_test_count = spark.table(r2_source).count()\n",
        "            print(f\"✅ R2 source accessible: {r2_test_count} records\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Cannot access R2 source table: {str(e)}\")\n",
        "        \n",
        "        # Detect changes\n",
        "        change_info = detect_r2_changes(r2_source, local_target)\n",
        "        \n",
        "        if change_info.get('error'):\n",
        "            raise Exception(f\"Change detection failed: {change_info['error']}\")\n",
        "        \n",
        "        # Synchronize if changes detected\n",
        "        if change_info.get('has_changes', False):\n",
        "            print(f\"📊 Changes detected: {change_info['total_changes']} records\")\n",
        "            \n",
        "            sync_result = sync_r2_to_local(r2_source, local_target)\n",
        "            \n",
        "            if sync_result.get('error'):\n",
        "                raise Exception(f\"Synchronization failed: {sync_result['error']}\")\n",
        "            \n",
        "            job_end = datetime.now()\n",
        "            duration = (job_end - job_start).total_seconds()\n",
        "            \n",
        "            # Log successful sync\n",
        "            log_entry = {\n",
        "                'timestamp': job_start.isoformat(),\n",
        "                'duration_seconds': duration,\n",
        "                'status': 'success_with_changes',\n",
        "                'records_processed': sync_result['records_processed'],\n",
        "                'new_records': sync_result.get('new_records', 0),\n",
        "                'updated_records': sync_result.get('updated_records', 0),\n",
        "                'final_count': sync_result.get('final_count', 0),\n",
        "                'r2_version': sync_result.get('r2_version', 0),\n",
        "                'r2_source': r2_source,\n",
        "                'local_target': local_target\n",
        "            }\n",
        "            \n",
        "            print(f\"✅ Sync completed successfully\")\n",
        "            print(f\"   Duration: {duration:.2f} seconds\")\n",
        "            print(f\"   Records processed: {sync_result['records_processed']}\")\n",
        "            print(f\"   New: {sync_result.get('new_records', 0)}, Updated: {sync_result.get('updated_records', 0)}\")\n",
        "            \n",
        "        else:\n",
        "            job_end = datetime.now()\n",
        "            duration = (job_end - job_start).total_seconds()\n",
        "            \n",
        "            log_entry = {\n",
        "                'timestamp': job_start.isoformat(),\n",
        "                'duration_seconds': duration,\n",
        "                'status': 'success_no_changes',\n",
        "                'records_processed': 0,\n",
        "                'message': 'No changes detected - local table is up to date',\n",
        "                'r2_source': r2_source,\n",
        "                'local_target': local_target\n",
        "            }\n",
        "            \n",
        "            print(f\"ℹ️  No changes detected - table is up to date\")\n",
        "            print(f\"   Duration: {duration:.2f} seconds\")\n",
        "        \n",
        "        # In production, you might want to:\n",
        "        # 1. Send metrics to monitoring system (DataDog, CloudWatch, etc.)\n",
        "        # 2. Log to centralized system (Splunk, ELK stack, etc.)\n",
        "        # 3. Update job metadata table\n",
        "        # 4. Send success notifications if needed\n",
        "        \n",
        "        return log_entry\n",
        "        \n",
        "    except Exception as e:\n",
        "        job_end = datetime.now()\n",
        "        duration = (job_end - job_start).total_seconds()\n",
        "        \n",
        "        error_log = {\n",
        "            'timestamp': job_start.isoformat(),\n",
        "            'duration_seconds': duration,\n",
        "            'status': 'error',\n",
        "            'error_message': str(e),\n",
        "            'r2_source': r2_source,\n",
        "            'local_target': local_target\n",
        "        }\n",
        "        \n",
        "        print(f\"❌ Job failed after {duration:.2f} seconds\")\n",
        "        print(f\"   Error: {str(e)}\")\n",
        "        \n",
        "        # In production:\n",
        "        # 1. Send alert to operations team\n",
        "        # 2. Log error details for debugging\n",
        "        # 3. Implement retry logic with exponential backoff\n",
        "        \n",
        "        raise\n",
        "\n",
        "# Test the production job\n",
        "job_result = production_sync_job()\n",
        "print(f\"\\n📋 Job Result Summary: {job_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Data Quality and Validation\n",
        "\n",
        "Implement data quality checks to ensure synchronization integrity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_sync_quality(r2_table, local_table):\n",
        "    \"\"\"\n",
        "    Validate data quality after synchronization.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Validation results\n",
        "    \"\"\"\n",
        "    validation_results = {\n",
        "        'passed': True,\n",
        "        'checks': [],\n",
        "        'warnings': [],\n",
        "        'errors': []\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Check 1: Record count consistency\n",
        "        r2_count = spark.table(r2_table).count()\n",
        "        local_count = spark.table(local_table).count()\n",
        "        \n",
        "        if r2_count == local_count:\n",
        "            validation_results['checks'].append(f\"✅ Record count match: {r2_count} records\")\n",
        "        else:\n",
        "            validation_results['errors'].append(f\"❌ Record count mismatch: R2={r2_count}, Local={local_count}\")\n",
        "            validation_results['passed'] = False\n",
        "        \n",
        "        # Check 2: Unique transaction IDs\n",
        "        local_unique_check = spark.sql(f\"\"\"\n",
        "            SELECT COUNT(*) as total, COUNT(DISTINCT transaction_id) as unique_ids\n",
        "            FROM {local_table}\n",
        "        \"\"\").collect()[0]\n",
        "        \n",
        "        if local_unique_check[0] == local_unique_check[1]:\n",
        "            validation_results['checks'].append(f\"✅ All transaction IDs are unique\")\n",
        "        else:\n",
        "            validation_results['errors'].append(f\"❌ Duplicate transaction IDs detected\")\n",
        "            validation_results['passed'] = False\n",
        "        \n",
        "        # Check 3: Data freshness (SCD metadata)\n",
        "        freshness_check = spark.sql(f\"\"\"\n",
        "            SELECT \n",
        "                DATEDIFF(HOUR, MAX(sync_timestamp), current_timestamp()) as hours_since_sync,\n",
        "                MAX(source_version) as latest_version\n",
        "            FROM {local_table}\n",
        "        \"\"\").collect()[0]\n",
        "        \n",
        "        hours_since_sync = freshness_check[0]\n",
        "        if hours_since_sync <= 24:  # Data should be less than 24 hours old\n",
        "            validation_results['checks'].append(f\"✅ Data is fresh: {hours_since_sync} hours old\")\n",
        "        else:\n",
        "            validation_results['warnings'].append(f\"⚠️  Data may be stale: {hours_since_sync} hours old\")\n",
        "        \n",
        "        # Check 4: Amount field validation (no negative values)\n",
        "        negative_amounts = spark.sql(f\"\"\"\n",
        "            SELECT COUNT(*) as negative_count\n",
        "            FROM {local_table}\n",
        "            WHERE amount < 0\n",
        "        \"\"\").collect()[0][0]\n",
        "        \n",
        "        if negative_amounts == 0:\n",
        "            validation_results['checks'].append(f\"✅ No negative amounts found\")\n",
        "        else:\n",
        "            validation_results['warnings'].append(f\"⚠️  Found {negative_amounts} negative amounts\")\n",
        "        \n",
        "        # Check 5: Date range validation\n",
        "        date_range = spark.sql(f\"\"\"\n",
        "            SELECT \n",
        "                MIN(transaction_date) as earliest_date,\n",
        "                MAX(transaction_date) as latest_date,\n",
        "                COUNT(DISTINCT transaction_date) as distinct_dates\n",
        "            FROM {local_table}\n",
        "        \"\"\").collect()[0]\n",
        "        \n",
        "        validation_results['checks'].append(f\"✅ Date range: {date_range[0]} to {date_range[1]} ({date_range[2]} distinct dates)\")\n",
        "        \n",
        "        return validation_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        validation_results['errors'].append(f\"❌ Validation failed: {str(e)}\")\n",
        "        validation_results['passed'] = False\n",
        "        return validation_results\n",
        "\n",
        "# Run validation\n",
        "validation = validate_sync_quality(\n",
        "    f\"{DA.catalog}.{DA.schema}.r2_source_data\",\n",
        "    local_table_name\n",
        ")\n",
        "\n",
        "print(\"🔍 Data Quality Validation Results:\")\n",
        "print(f\"\\n   Overall Status: {'✅ PASSED' if validation['passed'] else '❌ FAILED'}\")\n",
        "\n",
        "if validation['checks']:\n",
        "    print(\"\\n   ✅ Passed Checks:\")\n",
        "    for check in validation['checks']:\n",
        "        print(f\"      {check}\")\n",
        "\n",
        "if validation['warnings']:\n",
        "    print(\"\\n   ⚠️  Warnings:\")\n",
        "    for warning in validation['warnings']:\n",
        "        print(f\"      {warning}\")\n",
        "\n",
        "if validation['errors']:\n",
        "    print(\"\\n   ❌ Errors:\")\n",
        "    for error in validation['errors']:\n",
        "        print(f\"      {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Performance Analysis\n",
        "\n",
        "Analyze the performance characteristics of our R2-based synchronization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance analysis\n",
        "performance_stats = spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    'Local Synchronized Table' as source,\n",
        "    COUNT(*) as total_records,\n",
        "    ROUND(SUM(amount), 2) as total_amount,\n",
        "    ROUND(AVG(amount), 2) as avg_amount,\n",
        "    COUNT(DISTINCT customer_id) as unique_customers,\n",
        "    COUNT(DISTINCT region) as unique_regions,\n",
        "    MIN(transaction_date) as earliest_date,\n",
        "    MAX(transaction_date) as latest_date,\n",
        "    MAX(source_version) as latest_version,\n",
        "    MAX(sync_timestamp) as last_sync\n",
        "FROM {local_table_name}\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'R2 External Table' as source,\n",
        "    COUNT(*) as total_records,\n",
        "    ROUND(SUM(amount), 2) as total_amount,\n",
        "    ROUND(AVG(amount), 2) as avg_amount,\n",
        "    COUNT(DISTINCT customer_id) as unique_customers,\n",
        "    COUNT(DISTINCT region) as unique_regions,\n",
        "    MIN(transaction_date) as earliest_date,\n",
        "    MAX(transaction_date) as latest_date,\n",
        "    NULL as latest_version,\n",
        "    NULL as last_sync\n",
        "FROM {DA.catalog}.{DA.schema}.r2_source_data\n",
        "\"\"\")\n",
        "\n",
        "print(\"📊 Performance Comparison:\")\n",
        "display(performance_stats)\n",
        "\n",
        "# Check table sizes and optimization\n",
        "print(\"\\n🔧 Table Details:\")\n",
        "local_details = spark.sql(f\"DESCRIBE DETAIL {local_table_name}\")\n",
        "display(local_details.select(\"name\", \"numFiles\", \"sizeInBytes\", \"properties\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "🎉 **Congratulations!** You have successfully implemented a recipient-side synchronization system for cross-cloud data replication using Cloudflare R2.\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "✅ **R2 Access**: Connected to Cloudflare R2 external table with zero egress costs  \n",
        "✅ **Local Table**: Created managed table with Type 1 SCD metadata  \n",
        "✅ **Change Detection**: Implemented automatic change detection from R2 source  \n",
        "✅ **MERGE Operations**: Used MERGE for efficient Type 1 SCD updates  \n",
        "✅ **Production Job**: Created schedulable synchronization function  \n",
        "✅ **Data Quality**: Implemented comprehensive validation checks  \n",
        "✅ **Monitoring**: Added performance analysis and health monitoring  \n",
        "\n",
        "### Key Benefits Achieved:\n",
        "\n",
        "🌍 **Global Access**: Fast data access from Cloudflare's global network  \n",
        "💰 **Cost Efficient**: Zero egress fees for reading from R2  \n",
        "⚡ **Local Performance**: Optimized local queries on synchronized data  \n",
        "🔄 **Automatic Updates**: MERGE-based Type 1 SCD for current data  \n",
        "📊 **Data Quality**: Built-in validation and monitoring  \n",
        "🛡️ **Reliability**: Error handling and retry capabilities  \n",
        "\n",
        "### Type 1 SCD Implementation:\n",
        "\n",
        "Our implementation provides:\n",
        "- **Current Data Only**: Type 1 SCD overwrites old values with new ones\n",
        "- **Metadata Tracking**: `last_updated_at`, `source_version`, `sync_timestamp`\n",
        "- **MERGE Logic**: Efficient upsert operations (UPDATE existing, INSERT new)\n",
        "- **Change Detection**: Automatic identification of new and updated records\n",
        "\n",
        "### Next Steps for Production:\n",
        "\n",
        "1. **Schedule the Job**: \n",
        "   - Create Databricks Job with the `production_sync_job()` function\n",
        "   - Set appropriate frequency (hourly, daily, etc.)\n",
        "   - Configure cluster auto-scaling for cost efficiency\n",
        "\n",
        "2. **Security & Secrets**:\n",
        "   - Store R2 credentials in Databricks Secrets\n",
        "   - Implement row-level security if needed\n",
        "   - Set up audit logging\n",
        "\n",
        "3. **Advanced Monitoring**:\n",
        "   - Set up alerts for sync failures\n",
        "   - Create dashboards for data freshness\n",
        "   - Implement SLA monitoring\n",
        "\n",
        "4. **Performance Optimization**:\n",
        "   - Implement partition pruning strategies\n",
        "   - Use table optimization (OPTIMIZE, VACUUM)\n",
        "   - Consider Z-ordering for better query performance\n",
        "\n",
        "5. **Advanced SCD Patterns**:\n",
        "   - Implement Type 2 SCD if historical tracking needed\n",
        "   - Add soft deletes handling\n",
        "   - Implement CDC (Change Data Capture) patterns\n",
        "\n",
        "### Cost Benefits:\n",
        "\n",
        "By using Cloudflare R2 for cross-cloud replication:\n",
        "- **Traditional**: $90/TB egress costs for cross-cloud data access\n",
        "- **With R2**: $0/TB egress costs + $15/TB storage\n",
        "- **Savings**: Up to 95% reduction in data sharing costs\n",
        "\n",
        "### Use Cases:\n",
        "\n",
        "This pattern is ideal for:\n",
        "- Multi-cloud analytics environments\n",
        "- Global data distribution scenarios\n",
        "- Cost-sensitive data sharing requirements\n",
        "- SaaS platforms serving global customers\n",
        "- Backup and disaster recovery strategies\n",
        "\n",
        "The recipient now has a robust, cost-effective way to stay synchronized with provider data across cloud boundaries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "3.1 DEMO Cross Cloud Replication with Cloudflare R2 [Recipient]",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
